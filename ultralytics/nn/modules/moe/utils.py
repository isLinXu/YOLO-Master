# ðŸ§Please note that this file has been modified by Tencent on 2026/01/18. All Tencent Modifications are Copyright (C) 2026 Tencent.
"""Utility functions for Mixture-of-Experts models"""
import torch
import torch.nn as nn
from typing import Tuple, Union, List


def get_safe_groups(channels: int, desired_groups: int = 8) -> int:
    """Ensure num_groups divides channels"""
    groups = min(desired_groups, channels)
    while channels % groups != 0:
        groups -= 1
    return max(1, groups)


# ==========================================
# Utility: FLOPs calculator (optimized)
# ==========================================
class FlopsUtils:
    @staticmethod
    def count_conv2d(layer: Union[nn.Conv2d, nn.Sequential], input_shape: Tuple[int, int, int, int]) -> float:
        B, C, H, W = input_shape
        if isinstance(layer, nn.Sequential):
            total = 0
            curr_shape = input_shape
            for m in layer:
                if isinstance(m, nn.Conv2d):
                    total += FlopsUtils.count_conv2d(m, curr_shape)
                    # Simple shape derivation
                    curr_h = int((curr_shape[2] + 2 * m.padding[0] - m.kernel_size[0]) / m.stride[0] + 1)
                    curr_w = int((curr_shape[3] + 2 * m.padding[1] - m.kernel_size[1]) / m.stride[1] + 1)
                    curr_shape = (B, m.out_channels, curr_h, curr_w)
            return total

        # Single Conv2d compute
        out_h = (H + 2 * layer.padding[0] - layer.dilation[0] * (layer.kernel_size[0] - 1) - 1) // layer.stride[0] + 1
        out_w = (W + 2 * layer.padding[1] - layer.dilation[1] * (layer.kernel_size[1] - 1) - 1) // layer.stride[1] + 1
        ops = (layer.in_channels // layer.groups) * layer.kernel_size[0] * layer.kernel_size[1]
        ops = (ops + (1 if layer.bias is not None else 0)) * layer.out_channels * out_h * out_w
        return ops * 2.0 * B


# ==========================================
# Batched expert computation (key optimization)
# ==========================================
class BatchedExpertComputation:
    """
    Strategy: batch expert computations to eliminate for-loops.
    Performance: ~3â€“5x inference speedup observed.
    """

    @staticmethod
    def compute_sparse_experts_batched(
            x: torch.Tensor,
            experts: nn.ModuleList,
            routing_weights: torch.Tensor,
            routing_indices: torch.Tensor,
            top_k: int,
            num_experts: int
    ) -> torch.Tensor:
        """
        Batched expert computation:
        1) Pre-allocate outputs for all experts
        2) Compute all activated experts in parallel
        3) Aggregate using efficient scatter/index_add
        """
        B, C, H, W = x.shape
        out_channels = experts[0].conv[-2].out_channels if hasattr(experts[0], 'conv') else experts[0].primary_conv[
            0].out_channels

        # Flatten indices and weights
        indices_flat = routing_indices.view(B, top_k).squeeze(-1).squeeze(-1)  # [B, top_k]
        weights_flat = routing_weights.view(B, top_k).squeeze(-1).squeeze(-1)  # [B, top_k]

        # Plan A: conditional computation (skip low-weight experts)
        # Threshold is tunable (accuracy vs speed)
        weight_threshold = 0.01
        valid_mask = weights_flat > weight_threshold

        # Initialize outputs
        expert_output = torch.zeros(B, out_channels, H, W, device=x.device, dtype=x.dtype)

        # Plan B: parallel batching (recommended)
        # Collect all samples per expert
        for expert_idx in range(num_experts):
            # Find all (batch, k) positions that selected this expert
            expert_mask = (indices_flat == expert_idx) & valid_mask

            if not expert_mask.any():
                continue

            # Get batch indices and corresponding weights
            batch_indices, k_indices = torch.where(expert_mask)

            # Batched forward pass
            expert_input = x[batch_indices]
            expert_out = experts[expert_idx](expert_input)

            # Apply weights
            weights = weights_flat[batch_indices, k_indices].view(-1, 1, 1, 1)
            weighted_out = expert_out * weights

            # Accumulate outputs (efficient index_add_)
            expert_output.index_add_(0, batch_indices, weighted_out.to(expert_output.dtype))

        return expert_output
